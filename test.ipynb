{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rlglue import rl_glue as RLGlue\n",
    "from agents import QLearningAgent # import agent\n",
    "from environments import TwoChoiceMDP # import environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-57fdd28bbec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_episodes\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;31m# Runs an episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mrl_glue1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Runs an episode while keeping track of visited states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/bigyan/RL1/rlglue/rl_glue.py\u001b[0m in \u001b[0;36mrl_episode\u001b[0;34m(self, max_steps_this_episode)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mis_terminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         while (not is_terminal) and ((max_steps_this_episode == 0) or\n",
      "\u001b[0;32m~/Desktop/bigyan/RL1/rlglue/rl_glue.py\u001b[0m in \u001b[0;36mrl_start\u001b[0;34m(self, agent_start_info, env_start_info)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/bigyan/RL1/agents/QLearningAgent.py\u001b[0m in \u001b[0;36magent_start\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Choose action using epsilon greedy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mcurrent_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "agents = {\n",
    "    \"Q-learning\": QLearningAgent,\n",
    "}\n",
    "env = TwoChoiceMDP\n",
    "all_reward_sums = {} # Contains sum of rewards during episode\n",
    "all_state_visits = {} # Contains state visit counts during the last 10 episodes\n",
    "agent_info = {\"num_actions\": 2, \"num_states\": 9, \"epsilon\": 0.1, \"step_size\": 0.5, \"discount\": 0.5}\n",
    "env_info = {}\n",
    "num_runs = 100 # The number of runs\n",
    "num_episodes = 100 # The number of episodes in each run\n",
    "\n",
    "for algorithm in [\"Q-learning\"]:\n",
    "    all_reward_sums[algorithm] = []\n",
    "    all_state_visits[algorithm] = []\n",
    "    for run in tqdm(range(num_runs)):\n",
    "        agent_info[\"seed\"] = run\n",
    "        rl_glue1 = RLGlue.RLGlue(env, agents[algorithm])\n",
    "        rl_glue1.rl_init(agent_info, env_info)\n",
    "\n",
    "        reward_sums = []\n",
    "        state_visits = np.zeros(8)\n",
    "        for episode in range(num_episodes):\n",
    "            if episode < num_episodes - 10:\n",
    "                # Runs an episode\n",
    "                rl_glue1.rl_episode(10000) \n",
    "            else: \n",
    "                # Runs an episode while keeping track of visited states\n",
    "                state, action = rl_glue.rl_start()\n",
    "                state_visits[state] += 1\n",
    "                is_terminal = False\n",
    "                while not is_terminal:\n",
    "                    reward, state, action, is_terminal = rl_glue.rl_step()\n",
    "                    state_visits[state] += 1\n",
    "                \n",
    "            reward_sums.append(rl_glue.rl_return())\n",
    "            last_episode_total_reward = rl_glue.rl_return()\n",
    "            \n",
    "        all_reward_sums[algorithm].append(reward_sums)\n",
    "        all_state_visits[algorithm].append(state_visits)\n",
    "\n",
    "# plot results\n",
    "for algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n",
    "    plt.plot(np.mean(all_reward_sums[algorithm], axis=0), label=algorithm)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\",rotation=0, labelpad=40)\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(-100,0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
